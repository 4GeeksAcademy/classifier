{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "def dummy_npwarn_decorator_factory():\n",
                "  def npwarn_decorator(x):\n",
                "    return x\n",
                "  return npwarn_decorator\n",
                "np._no_nep50_warning = getattr(np, '_no_nep50_warning', dummy_npwarn_decorator_factory)\n",
                "import torch as to\n",
                "import torch.nn as nn\n",
                "import pandas as pd\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "import torchvision.transforms as transforms\n",
                "from PIL import Image\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "model = Sequential()\n",
                "model.add(Conv2D(input_shape = (224,224,3), filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 64,kernel_size = (3,3),padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(units = 4096,activation = \"relu\"))\n",
                "model.add(Dense(units = 4096,activation = \"relu\"))\n",
                "model.add(Dense(units = 2, activation = \"softmax\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Trimmed-down model is ready for training!\n"
                    ]
                }
            ],
            "source": [
                "import torch.nn as nn\n",
                "import torch\n",
                "\n",
                "# Define a simpler CNN model using Sequential\n",
                "model = nn.Sequential(\n",
                "    # Block 1\n",
                "    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(kernel_size=2, stride=2),  # 224 → 112\n",
                "\n",
                "    # Block 2\n",
                "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(kernel_size=2, stride=2),  # 112 → 56\n",
                "\n",
                "    # Block 3\n",
                "    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(kernel_size=2, stride=2),  # 56 → 28\n",
                "\n",
                "    # Block 4 (Final Conv Layer)\n",
                "    nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(kernel_size=2, stride=2),  # 28 → 14\n",
                "\n",
                "    # Flatten\n",
                "    nn.Flatten(),\n",
                "\n",
                "    # Fully Connected Layers (Reduced Size)\n",
                "    nn.Linear(in_features=256 * 14 * 14, out_features=128),  # Reduced from 512x7x7\n",
                "    nn.ReLU(),\n",
                "    nn.Linear(128, 64),\n",
                "    nn.ReLU(),\n",
                "    nn.Linear(64, 2)  # Binary Classification (Dog vs Cat)\n",
                ")\n",
                "\n",
                "# Move model to GPU if available\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model.to(device)\n",
                "\n",
                "print(\"Trimmed-down model is ready for training!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model successfully built with Sequential!\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "# Define a function to compute the correct input size for Linear layer\n",
                "def get_flattened_size():\n",
                "    with torch.no_grad():\n",
                "        dummy_input = torch.zeros(1, 3, 224, 224)  # (batch_size=1, channels=3, height=224, width=224)\n",
                "        conv_output = conv_layers(dummy_input)\n",
                "        return conv_output.view(1, -1).size(1)  # Flatten and get size\n",
                "\n",
                "# Define CNN Layers (Convolutional + MaxPooling)\n",
                "conv_layers = nn.Sequential(\n",
                "    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(kernel_size=2, stride=2),  # 224 → 112\n",
                "\n",
                "    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(kernel_size=2, stride=2),  # 112 → 56\n",
                "\n",
                "    nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(kernel_size=2, stride=2),  # 56 → 28\n",
                "\n",
                "    nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(kernel_size=2, stride=2),  # 28 → 14\n",
                "\n",
                "    nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
                "    nn.ReLU(),\n",
                "    nn.MaxPool2d(kernel_size=2, stride=2)  # 14 → 7\n",
                ")\n",
                "\n",
                "# Compute the correct flattened size\n",
                "flattened_size = get_flattened_size()\n",
                "\n",
                "# Define Fully Connected Layers\n",
                "fc_layers = nn.Sequential(\n",
                "    nn.Linear(in_features=flattened_size, out_features=512),  # Corrected in_features\n",
                "    nn.ReLU(),\n",
                "    nn.Linear(512, 512),\n",
                "    nn.ReLU(),\n",
                "    nn.Linear(512, 2)  # 2 classes (Dog vs. Cat)\n",
                ")\n",
                "\n",
                "# Combine Everything into `Sequential`\n",
                "model = nn.Sequential(\n",
                "    conv_layers,\n",
                "    nn.Flatten(),  # Flattens the output before FC layers\n",
                "    fc_layers\n",
                ")\n",
                "\n",
                "# Move model to GPU (if available)\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model.to(device)\n",
                "\n",
                "print(\"Model successfully built with Sequential!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Batch of images shape: torch.Size([32, 3, 224, 224])\n",
                        "Batch of labels: tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
                        "        0, 0, 0, 1, 0, 1, 0, 0])\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import torch\n",
                "import torchvision.transforms as transforms\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "\n",
                "# Define device (use GPU if available)\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "# Define image transformations\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),  # Resize images\n",
                "    transforms.ToTensor(),  # Convert images to Tensors\n",
                "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values\n",
                "])\n",
                "\n",
                "# Custom Dataset Class for Ordered Cats & Dogs Images\n",
                "class CatsDogsDataset(Dataset):\n",
                "    def __init__(self, folder_path, transform=None):\n",
                "        self.folder_path = folder_path\n",
                "        self.transform = transform\n",
                "        self.image_files = sorted(\n",
                "            [f for f in os.listdir(folder_path) if f.lower().endswith(('jpg', 'jpeg', 'png'))]\n",
                "        )\n",
                "\n",
                "        # First 12,500 images are cats (label=0), last 12,500 are dogs (label=1)\n",
                "        self.labels = [0] * 12500 + [1] * 12500\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.image_files)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_name = self.image_files[idx]\n",
                "        img_path = os.path.join(self.folder_path, img_name)\n",
                "        image = Image.open(img_path).convert(\"RGB\")\n",
                "\n",
                "        label = self.labels[idx]  # Assign label based on index\n",
                "\n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "\n",
                "        return image, label\n",
                "\n",
                "# Define dataset paths\n",
                "train_path = \"/workspaces/classifier/dogs-vs-cats/train\"\n",
                "test_path = \"/workspaces/classifier/dogs-vs-cats/test1\"\n",
                "\n",
                "# Create Dataset and DataLoader\n",
                "train_dataset = CatsDogsDataset(train_path, transform=transform)\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
                "\n",
                "test_dataset = CatsDogsDataset(test_path, transform=transform)\n",
                "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
                "\n",
                "# Print sample data\n",
                "for images, labels in train_loader:\n",
                "    print(f\"Batch of images shape: {images.shape}\")  # Should be (batch_size, 3, 224, 224)\n",
                "    print(f\"Batch of labels: {labels}\")  # Should be 0 for Cats, 1 for Dogs\n",
                "    break  # Print one batch and exit\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total images loaded for training: 5000\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "device = to.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "# Define image transformations\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),  # Resize images\n",
                "    transforms.ToTensor(),  # Convert images to Tensors\n",
                "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values\n",
                "])\n",
                "\n",
                "# Custom Dataset Class to Load Only 5,000 Images (2,500 Cats + 2,500 Dogs)\n",
                "class CatsDogsSubsetDataset(Dataset):\n",
                "    def __init__(self, folder_path, transform=None, subset_size=5000):\n",
                "        self.folder_path = folder_path\n",
                "        self.transform = transform\n",
                "        self.image_files = sorted(\n",
                "            [f for f in os.listdir(folder_path) if f.lower().endswith(('jpg', 'jpeg', 'png'))]\n",
                "        )\n",
                "\n",
                "        # Load only the first 2500 cats and 2500 dogs\n",
                "        self.image_files = self.image_files[:2500] + self.image_files[12500:15000]  # 2500 Cats + 2500 Dogs\n",
                "\n",
                "        # Assign labels: 0 for Cats, 1 for Dogs\n",
                "        self.labels = [0] * 2500 + [1] * 2500  \n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.image_files)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_name = self.image_files[idx]\n",
                "        img_path = os.path.join(self.folder_path, img_name)\n",
                "        image = Image.open(img_path).convert(\"RGB\")\n",
                "\n",
                "        label = self.labels[idx]  # Assign label based on index\n",
                "\n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "\n",
                "        return image, label\n",
                "\n",
                "# Define dataset path\n",
                "train_path = \"/workspaces/classifier/dogs-vs-cats/train\"\n",
                "\n",
                "# Create Dataset and DataLoader for Only 5,000 Images\n",
                "train_dataset = CatsDogsSubsetDataset(train_path, transform=transform)\n",
                "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
                "\n",
                "# Print dataset size\n",
                "print(f\"Total images loaded for training: {len(train_dataset)}\")  # Should print 5000\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/10, Loss: 0.6467949376700405\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Define Loss and Optimizer\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
                "\n",
                "# Training Loop\n",
                "num_epochs = 10\n",
                "for epoch in range(num_epochs):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "\n",
                "    for images, labels in train_loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "\n",
                "        # Forward pass\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "\n",
                "        # Backpropagation\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        running_loss += loss.item()\n",
                "\n",
                "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
                "\n",
                "print(\"Training Complete!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
